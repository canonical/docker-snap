#!/bin/bash

set -eu

U_MACHINE="$(uname -m)"
U_OS="$(uname -o)"
U_KERNEL="${U_OS##*/}"
U_USERLAND="${U_OS%%/*}"

ARCH_TRIPLET="${U_MACHINE}-${U_KERNEL,,}-${U_USERLAND,,}"

device_wait() {

    COUNT=0
    SLEEP=3
    TRIES=10

    echo "Waiting for device to become available: ${1}"

    while [ ${COUNT} -le ${TRIES} ] ; do
        echo "Checking device: ${COUNT}/${TRIES}"
        test -c "${1}"
        if [ $? -eq 0 ] ; then
            echo "Device found"
            return 0
        fi
        sleep $SLEEP
        COUNT=$(($COUNT + 1))
    done

    echo "Device not found"
    return 1

}

# Generate the CDI config #
cdi_generate () {
    # Allow configred device-name-strategy, or default to index [ default in nvidia-ctk ] #
    CDI_DEVICE_NAME_STRATEGY="$(snapctl get nvidia-support.cdi.device-name-strategy)"
    CDI_DEVICE_NAME_STRATEGY="${CDI_DEVICE_NAME_STRATEGY:-index}"

    "${SNAP}/usr/bin/nvidia-ctk" cdi generate --nvidia-ctk-path "${SNAP}/usr/bin/nvidia-ctk" --library-search-path "${SNAP}/graphics/lib/${ARCH_TRIPLET}" --device-name-strategy "${CDI_DEVICE_NAME_STRATEGY}" --output "${SNAP_DATA}/etc/cdi/nvidia.yaml"
}

# Create the nvidia runtime config, either snap default or custom #
nvidia_runtime_config () {
    RUNTIME_CONFIG_OVERRIDE="$(snapctl get nvidia-support.runtime.config-override)"

    # Custom #
    if [ -n "${RUNTIME_CONFIG_OVERRIDE}" ] ; then
        echo "${RUNTIME_CONFIG_OVERRIDE}" > "${SNAP_DATA}/etc/nvidia-container-runtime/config.toml"
    # Default - opinionated, but most viable option for now #
    else
        rm -f "${SNAP_DATA}/etc/nvidia-container-runtime/config.toml"
        "${SNAP}/usr/bin/nvidia-ctk" config --in-place --set nvidia-container-runtime.mode=cdi
    fi
}

# Generate the dockerd runtime config #
docker_runtime_configure () {
    "${SNAP}/usr/bin/nvidia-ctk" runtime configure --runtime=docker --runtime-path "${SNAP}/usr/bin/nvidia-container-runtime" --config "${SNAP_DATA}/config/daemon.json"
}

# Setup failure recovery #
setup_fail () {
    echo "WARNING: Conainter Toolkit setup seemed to fail with an error"

    # Remove nvidia runtime config, if it exists #
    jq -r 'del(.runtimes.nvidia)' "${SNAP_DATA}/config/daemon.json" > "${SNAP_DATA}/config/daemon.json.new"

    # If it was removed [ there was a change ], copy in the new config, remove CDI config,  and set service restart flag #
    if ! cmp "${SNAP_DATA}/config/daemon.json"{,.new} >/dev/null ; then
        mv "${SNAP_DATA}/config/daemon.json"{.new,}
        rm -f "${SNAP_DATA}/etc/cdi/nvidia.yaml"
        rm -f "${SNAP_DATA}/etc/nvidia-container-runtime/config.toml"
    fi
}

# Info #
setup_info () {
    echo "Conainter Toolkit setup complete"
}

# Just exit if NVIDIA support is disabled #
NVIDIA_SUPPORT_DISABLED="$(snapctl get nvidia-support.disabled)"
[ "${NVIDIA_SUPPORT_DISABLED}" != "true" ] || exit 0

# Ensure nvidia support setup correctly, and only if hardware is preset and correct #
if snapctl is-connected graphics-core22 ; then

    # Connection hooks are run early - copy the config file from $SNAP into $SNAP_DATA if it doesn't exist
    if [ ! -f "$SNAP_DATA/config/daemon.json" ]; then
        mkdir -p "$SNAP_DATA/config"
        cp "$SNAP/config/daemon.json" "$SNAP_DATA/config/daemon.json"
    fi

    # Check if hardware is present - just exit if not #
    lspci -d 10de: | grep -q 'NVIDIA Corporation' || exit 0
    echo "NVIDIA hardware detected: $(lspci -d 10de:)"

    # As service order is not guaranteed outside of snap - wait a bit for nvidia assemble to complete #
    device_wait /dev/nvidiactl || exit 0
    echo "NVIDIA ready"

    # Ensure the layout dirs for cdi exists #
    mkdir -p "${SNAP_DATA}/etc/cdi"
    mkdir -p "${SNAP_DATA}/etc/nvidia-container-runtime"

    # Setup nvidia support, but do not exit on failure #
    cdi_generate && nvidia_runtime_config && docker_runtime_configure && setup_info || setup_fail

fi
